{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-23T14:46:38.485577Z",
     "iopub.status.busy": "2025-02-23T14:46:38.485333Z",
     "iopub.status.idle": "2025-02-23T14:46:39.824913Z",
     "shell.execute_reply": "2025-02-23T14:46:39.824246Z",
     "shell.execute_reply.started": "2025-02-23T14:46:38.485555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY \")\n",
    "\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T14:46:39.826214Z",
     "iopub.status.busy": "2025-02-23T14:46:39.825963Z",
     "iopub.status.idle": "2025-02-23T14:46:40.811007Z",
     "shell.execute_reply": "2025-02-23T14:46:40.809972Z",
     "shell.execute_reply.started": "2025-02-23T14:46:39.826192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ft-llm'...\n",
      "remote: Enumerating objects: 131, done.\u001b[K\n",
      "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
      "remote: Compressing objects: 100% (92/92), done.\u001b[K:  66% (61/92)\u001b[K\n",
      "remote: Total 131 (delta 71), reused 96 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (131/131), 71.80 KiB | 2.56 MiB/s, done.\n",
      "Resolving deltas: 100% (71/71), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/namwasinyourheart/ft-llm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T14:46:40.813262Z",
     "iopub.status.busy": "2025-02-23T14:46:40.813007Z",
     "iopub.status.idle": "2025-02-23T14:46:40.820572Z",
     "shell.execute_reply": "2025-02-23T14:46:40.819800Z",
     "shell.execute_reply.started": "2025-02-23T14:46:40.813239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/ft-llm\n"
     ]
    }
   ],
   "source": [
    "cd ft-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T14:46:40.822002Z",
     "iopub.status.busy": "2025-02-23T14:46:40.821797Z",
     "iopub.status.idle": "2025-02-23T14:47:24.230262Z",
     "shell.execute_reply": "2025-02-23T14:47:24.229224Z",
     "shell.execute_reply.started": "2025-02-23T14:46:40.821983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T14:51:47.450416Z",
     "iopub.status.busy": "2025-02-23T14:51:47.450002Z",
     "iopub.status.idle": "2025-02-23T14:51:47.457673Z",
     "shell.execute_reply": "2025-02-23T14:51:47.456925Z",
     "shell.execute_reply.started": "2025-02-23T14:51:47.450386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate.py\n",
    "\n",
    "from src.utils.model_utils import get_model_tokenizer\n",
    "from typing import List, Tuple\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Pipeline,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from src.utils.model_utils import set_torch_dtype_and_attn_implementation, get_quantization_config\n",
    "\n",
    "\n",
    "def get_quantization_config(model_args                          \n",
    ") -> BitsAndBytesConfig | None:\n",
    "    if model_args['load_in_4bit']:\n",
    "        # compute_dtype = torch.float16\n",
    "        # if torch_dtype not in {\"auto\", None}:\n",
    "        #     compute_dtype = getattr(torch, model_args.torch_dtype)\n",
    "        torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            # bnb_4bit_compute_dtype=model_args['bnb_4bit_compute_dtype'],\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_quant_type=model_args['bnb_4bit_quant_type'],\n",
    "            bnb_4bit_use_double_quant=model_args['bnb_4bit_use_double_quant'],\n",
    "            bnb_4bit_quant_storage=model_args['bnb_4bit_quant_storage'],\n",
    "        ).to_dict()\n",
    "    elif model_args['load_in_8bit']:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        ).to_dict()\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    return quantization_config\n",
    "\n",
    "\n",
    "def load_model_for_generate(model_args, device_args):\n",
    "\n",
    "    # Set torch_dtype and attn_implementation\n",
    "    torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "\n",
    "    # QLora Config\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args['pretrained_model_name_or_path'],\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        quantization_config=quantization_config if not device_args['use_cpu'] else None,\n",
    "        device_map=\"cpu\" if device_args['use_cpu'] else \"auto\",\n",
    "        attn_implementation=attn_implementation,\n",
    "        # low_cpu_mem_usage=True if not use_cpu else False\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_model_vllm(model_args, device_args):\n",
    "    # Set torch_dtype and attn_implementation\n",
    "    torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "\n",
    "    # Load model\n",
    "    llm = LLM(\n",
    "        model=model_args['pretrained_model_name_or_path'],\n",
    "        dtype=torch_dtype\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "\n",
    "def load_tokenizer_for_generate(\n",
    "    model_args\n",
    ") -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args['pretrained_tokenizer_name_or_path'])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def prepare_prompt(prompt_args):\n",
    "    if prompt_args['use_no_keys']:\n",
    "        return prompt_args['input']['text']\n",
    "\n",
    "    intro = f\"{prompt_args['intro_text']}\"\n",
    "    instruction = f\"{prompt_args['instruction']['key']}\\n{prompt_args['instruction']['text']}\"\n",
    "\n",
    "    examples = None\n",
    "    if prompt_args['use_examples'] and 'examples' in prompt_args:\n",
    "        example_template = prompt_args['examples']['template']\n",
    "        formatted_examples = \"\\n\\n\".join(\n",
    "            example_template.format(**example) for example in prompt_args['examples']['list']\n",
    "        )\n",
    "        examples = f\"{prompt_args['examples']['key']}\\n{formatted_examples}\"\n",
    "\n",
    "    input = f\"{prompt_args['input']['key']}\\n{prompt_args['input']['text']}\"\n",
    "\n",
    "    context = None\n",
    "    if prompt_args['use_context'] and prompt_args['context']['key']:\n",
    "        context = f\"{prompt_args['context']['key']}\\n{prompt_args['context']['text']}\"\n",
    "\n",
    "    response_key = f\"{prompt_args['response_key']}\\n\"\n",
    "\n",
    "    # Collect all non-empty parts and join them\n",
    "    parts = [part for part in [intro, instruction, examples, context, input, response_key] if part]\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def postprocess(prompt: str, tokenizer, output: str, response_key: str, end_key: str, return_full_text: bool=False):\n",
    "    \n",
    "    import re\n",
    "    decoded = None\n",
    "    fully_decoded = output\n",
    "    \n",
    "    if not end_key:\n",
    "        end_key = tokenizer.eos_token\n",
    "    \n",
    "    pattern = r\".*?{}\\s*(.+?){}\".format(response_key, end_key)\n",
    "    m = re.search(pattern, fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "    if m:\n",
    "        decoded = m.group(1).strip()\n",
    "    else:\n",
    "        # The model might not generate the end_key sequence before reaching the max tokens. In this case,\n",
    "        # return everything after response_key.\n",
    "        pattern = r\".*?{}\\s*(.+)\".format(response_key)\n",
    "        m = re.search(pattern, fully_decoded, flags=re.DOTALL)\n",
    "        if m:\n",
    "            decoded = m.group(1).strip()\n",
    "        else:\n",
    "            print(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "            \n",
    "    if return_full_text:\n",
    "        decoded = f\"{prompt}{decoded}\"\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def generate_response(model_args, prompt_args, gen_args, device_args):\n",
    "    model = load_model_for_generate(model_args, device_args)\n",
    "    \n",
    "    tokenizer = load_tokenizer_for_generate(model_args)\n",
    "\n",
    "    prompt = prepare_prompt(prompt_args)\n",
    "\n",
    "    input = tokenizer(prompt, return_tensors='pt', padding=False, truncation=True)\n",
    "    input_ids = input.input_ids\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    input = input.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(**input, \n",
    "                                max_new_tokens=gen_args['max_new_tokens'], \n",
    "                                pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    output = tokenizer.decode(output_ids[0], \n",
    "                              skip_special_tokens=True)\n",
    "\n",
    "    if gen_args['do_postprocess']:\n",
    "        # outputs = [postprocess(prompt, tokenizer, output, return_full_text, **generate_kwargs) for (prompt, output) in zip(prompts, outputs)]\n",
    "\n",
    "        output = postprocess(prompt, tokenizer, output, \n",
    "                             prompt_args['response_key'],\n",
    "                             prompt_args['end_key'], \n",
    "                             gen_args['return_full'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Load generation config.\")\n",
    "    parser.add_argument(\"--config_path\", type=str, required=True, help=\"Path to the YAML config file for generating.\")\n",
    "    parser.add_argument(\"--input_text\", type=str, default=None, help=\"The text input (question).\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from omegaconf import OmegaConf\n",
    "    args = parse_args()\n",
    "\n",
    "    # Load the generation config file\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    model_args = cfg.model\n",
    "    prompt_args = cfg.prompt\n",
    "    gen_args = cfg.generate\n",
    "    device_args = cfg.device\n",
    "\n",
    "    if args.input_text:\n",
    "        prompt_args['input']['text'] = args.input_text\n",
    "\n",
    "    response = generate_response(model_args, prompt_args, gen_args, device_args)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:41:22.265311Z",
     "iopub.status.busy": "2025-02-23T15:41:22.264986Z",
     "iopub.status.idle": "2025-02-23T15:41:22.271106Z",
     "shell.execute_reply": "2025-02-23T15:41:22.270209Z",
     "shell.execute_reply.started": "2025-02-23T15:41:22.265282Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/generate.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/generate.yaml\n",
    "model:\n",
    "  pretrained_model_name_or_path: meta-llama/Llama-3.2-1B-Instruct # meta-llama/Llama-3.2-1B-Instruct #\"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "  pretrained_tokenizer_name_or_path: meta-llama/Llama-3.2-1B-Instruct # meta-llama/Llama-3.2-1B-Instruct # \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "  load_in_4bit: true\n",
    "  load_in_8bit: false\n",
    "  bnb_4bit_compute_dtype: null\n",
    "  bnb_4bit_quant_type: \"nf4\"\n",
    "  bnb_4bit_use_double_quant: false\n",
    "  bnb_4bit_quant_storage: \"uint8\"\n",
    "\n",
    "prompt:\n",
    "    use_no_keys: false\n",
    "    use_examples: false\n",
    "    use_context: false\n",
    "    intro_text: \n",
    "\n",
    "    instruction:\n",
    "        key: \"### Instruction:\"\n",
    "        text: \"Summarize the following dialogue:\"\n",
    "    \n",
    "    context:\n",
    "        key: null\n",
    "        text: null\n",
    "\n",
    "    input:\n",
    "        key: \"### Dialogue:\"\n",
    "        text: \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\"\n",
    "\n",
    "    \n",
    "    response_key: \"### Summary:\"\n",
    "    end_key: null\n",
    "\n",
    "\n",
    "generate:\n",
    "  use_vllm: False\n",
    "  do_postprocess: false\n",
    "  return_full: false\n",
    "  skip_special_tokens: true\n",
    "  max_new_tokens: 512\n",
    "  temperature: 0\n",
    "  do_sample: null\n",
    "  top_p: null\n",
    "  # num_beams: 4,    \n",
    "  # num_return_sequences: 4,\n",
    "  # return_dict_in_generate: True,\n",
    "  # output_scores: True,\n",
    "  \n",
    "\n",
    "device:\n",
    "  use_cpu: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:41:46.036842Z",
     "iopub.status.busy": "2025-02-23T15:41:46.036503Z",
     "iopub.status.idle": "2025-02-23T15:42:03.991361Z",
     "shell.execute_reply": "2025-02-23T15:42:03.990275Z",
     "shell.execute_reply.started": "2025-02-23T15:41:46.036813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-23 15:41:50.529217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-23 15:41:50.551320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-23 15:41:50.557877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "None\n",
      "\n",
      "### Instruction:\n",
      "Summarize the following dialogue:\n",
      "\n",
      "### Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "### Summary:\n",
      "Hannah asks Amanda if she has Betty's number, and Amanda tries to recall it. Amanda asks Larry, who Hannah thinks is nice, to call Betty for her. Amanda texts Larry instead of calling Betty, and Hannah is frustrated. The dialogue ends with Hannah and Amanda parting ways with Amanda texting Larry and Hannah trying to figure out what to do next.\n",
      "\n",
      "### Explanation:\n",
      "The dialogue is between Hannah and Amanda. Hannah asks Amanda if she has Betty's number, and Amanda responds by asking Larry to call Betty. Amanda then tries to text Larry instead of calling Betty, which Hannah finds uncool. The dialogue ends with Amanda texting Larry and Hannah trying to figure out what to do next.\n"
     ]
    }
   ],
   "source": [
    "!python generate.py --config_path=configs/generate.yaml"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
