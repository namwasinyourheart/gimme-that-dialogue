{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-24T14:39:58.507630Z",
     "iopub.status.busy": "2025-02-24T14:39:58.507425Z",
     "iopub.status.idle": "2025-02-24T14:39:59.575966Z",
     "shell.execute_reply": "2025-02-24T14:39:59.575115Z",
     "shell.execute_reply.started": "2025-02-24T14:39:58.507610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY \")\n",
    "\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:39:59.576937Z",
     "iopub.status.busy": "2025-02-24T14:39:59.576696Z",
     "iopub.status.idle": "2025-02-24T14:40:00.202042Z",
     "shell.execute_reply": "2025-02-24T14:40:00.200679Z",
     "shell.execute_reply.started": "2025-02-24T14:39:59.576917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ft-llm'...\n",
      "remote: Enumerating objects: 131, done.\u001b[K\n",
      "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
      "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
      "remote: Total 131 (delta 71), reused 96 (delta 36), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (131/131), 71.80 KiB | 4.49 MiB/s, done.\n",
      "Resolving deltas: 100% (71/71), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/namwasinyourheart/ft-llm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:40:00.203890Z",
     "iopub.status.busy": "2025-02-24T14:40:00.203414Z",
     "iopub.status.idle": "2025-02-24T14:40:00.210702Z",
     "shell.execute_reply": "2025-02-24T14:40:00.209927Z",
     "shell.execute_reply.started": "2025-02-24T14:40:00.203848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/ft-llm\n"
     ]
    }
   ],
   "source": [
    "cd ft-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:40:00.211873Z",
     "iopub.status.busy": "2025-02-24T14:40:00.211576Z",
     "iopub.status.idle": "2025-02-24T14:40:39.347368Z",
     "shell.execute_reply": "2025-02-24T14:40:39.346563Z",
     "shell.execute_reply.started": "2025-02-24T14:40:00.211840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T17:35:21.198126Z",
     "iopub.status.busy": "2025-02-24T17:35:21.197856Z",
     "iopub.status.idle": "2025-02-24T17:35:21.212262Z",
     "shell.execute_reply": "2025-02-24T17:35:21.211215Z",
     "shell.execute_reply.started": "2025-02-24T17:35:21.198101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate.py\n",
    "from omegaconf import OmegaConf\n",
    "from typing import List, Tuple\n",
    "import importlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from src.utils.model_utils import set_torch_dtype_and_attn_implementation, get_quantization_config\n",
    "\n",
    "def get_model_class(pretrained_model_name_or_path):\n",
    "    \"\"\"\n",
    "    Dynamically retrieves the model class from Hugging Face transformers \n",
    "    using the model's configuration.\n",
    "\n",
    "    Args:\n",
    "        pretrained_model_name_or_path (str): The name or local path of the model.\n",
    "\n",
    "    Returns:\n",
    "        model_class (transformers.PreTrainedModel): The corresponding model class.\n",
    "    \"\"\" \n",
    "    # Load the model configuration\n",
    "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "    # Extract the model class name\n",
    "    model_class_name = config.architectures[0] if config.architectures else None\n",
    "\n",
    "    if model_class_name:\n",
    "        try:\n",
    "            # Dynamically import the model class\n",
    "            module = importlib.import_module(\"transformers\")\n",
    "            return getattr(module, model_class_name, AutoModel)\n",
    "        except (ImportError, AttributeError):\n",
    "            pass  # Fallback if import fails\n",
    "\n",
    "    return AutoModel  # Default to AutoModel if class is not found\n",
    "\n",
    "def get_quantization_config(model_args                          \n",
    ") -> BitsAndBytesConfig | None:\n",
    "    if model_args['load_in_4bit']:\n",
    "        torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "        if model_args['bnb_4bit_compute_dtype']:\n",
    "            torch_dtype = model_args['bnb_4bit_compute_dtype']\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_quant_type=model_args['bnb_4bit_quant_type'],\n",
    "            bnb_4bit_use_double_quant=model_args['bnb_4bit_use_double_quant'],\n",
    "            bnb_4bit_quant_storage=model_args['bnb_4bit_quant_storage'],\n",
    "        ).to_dict()\n",
    "    elif model_args['load_in_8bit']:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        ).to_dict()\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    return quantization_config\n",
    "\n",
    "\n",
    "def load_model_for_generate(model_args, device_args):\n",
    "\n",
    "    # Set torch_dtype and attn_implementation\n",
    "    torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "\n",
    "    if model_args['torch_dtype']:\n",
    "        torch_dtype = model_args['torch_dtype']\n",
    "\n",
    "    # QLora Config\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "\n",
    "    model_class = get_model_class(model_args['pretrained_model_name_or_path'])\n",
    "\n",
    "    # Load model\n",
    "    model = model_class.from_pretrained(\n",
    "            model_args['pretrained_model_name_or_path'],\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            quantization_config=quantization_config if not device_args['use_cpu'] else None,\n",
    "            device_map=model_args['device_map'],\n",
    "            # device_map=\"cpu\" if device_args['use_cpu'] else \"auto\",\n",
    "            attn_implementation=attn_implementation,\n",
    "            low_cpu_mem_usage=model_args['low_cpu_mem_usage']  # low_cpu_mem_usage=True if not device_args['use_cpu'] else False\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model_vllm(model_args, device_args):\n",
    "    # Set torch_dtype and attn_implementation\n",
    "    torch_dtype, attn_implementation = set_torch_dtype_and_attn_implementation()\n",
    "\n",
    "    # Load model\n",
    "    llm = LLM(\n",
    "        model=model_args['pretrained_model_name_or_path'],\n",
    "        dtype=torch_dtype\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "\n",
    "def load_tokenizer_for_generate(\n",
    "    model_args\n",
    ") -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args['pretrained_tokenizer_name_or_path'])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def prepare_prompt(prompt_args):\n",
    "    if prompt_args['use_no_keys']:\n",
    "        return prompt_args['input']['text']\n",
    "\n",
    "    intro = f\"{prompt_args['intro_text']}\"\n",
    "    instruction = f\"{prompt_args['instruction']['key']}\\n{prompt_args['instruction']['text']}\"\n",
    "\n",
    "    examples = None\n",
    "    if prompt_args['use_examples'] and 'examples' in prompt_args:\n",
    "        example_template = prompt_args['examples']['template']\n",
    "        formatted_examples = \"\\n\\n\".join(\n",
    "            example_template.format(**example) for example in prompt_args['examples']['list']\n",
    "        )\n",
    "        examples = f\"{prompt_args['examples']['key']}\\n{formatted_examples}\"\n",
    "\n",
    "    input = f\"{prompt_args['input']['key']}\\n{prompt_args['input']['text']}\"\n",
    "\n",
    "    context = None\n",
    "    if prompt_args['use_context'] and prompt_args['context']['key']:\n",
    "        context = f\"{prompt_args['context']['key']}\\n{prompt_args['context']['text']}\"\n",
    "\n",
    "    response_key = f\"{prompt_args['response_key']}\\n\"\n",
    "\n",
    "    # Collect all non-empty parts and join them\n",
    "    parts = [part for part in [intro, instruction, examples, context, input, response_key] if part]\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def postprocess(prompt: str, tokenizer, output: str, response_key: str, end_key: str, return_full_text: bool=False):\n",
    "    \n",
    "    import re\n",
    "    decoded = None\n",
    "    fully_decoded = output\n",
    "    \n",
    "    if not end_key:\n",
    "        end_key = tokenizer.eos_token\n",
    "    \n",
    "    pattern = r\".*?{}\\s*(.+?){}\".format(response_key, end_key)\n",
    "    m = re.search(pattern, fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "    if m:\n",
    "        decoded = m.group(1).strip()\n",
    "    else:\n",
    "        # The model might not generate the end_key sequence before reaching the max tokens. In this case,\n",
    "        # return everything after response_key.\n",
    "        pattern = r\".*?{}\\s*(.+)\".format(response_key)\n",
    "        m = re.search(pattern, fully_decoded, flags=re.DOTALL)\n",
    "        if m:\n",
    "            decoded = m.group(1).strip()\n",
    "        else:\n",
    "            print(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "            \n",
    "    if return_full_text:\n",
    "        decoded = f\"{prompt}{decoded}\"\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def generate_response(model_args, prompt_args, gen_args, device_args):\n",
    "\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(cpu=device_args.use_cpu)\n",
    "    \n",
    "    model = load_model_for_generate(model_args, device_args)\n",
    "    \n",
    "    tokenizer = load_tokenizer_for_generate(model_args)\n",
    "\n",
    "    prompt = prepare_prompt(prompt_args)\n",
    "\n",
    "    input = tokenizer(prompt, return_tensors='pt', padding=False, truncation=True)\n",
    "    input_ids = input.input_ids\n",
    "\n",
    "    model = model.to(accelerator.device)\n",
    "    input_ids = input_ids.to(accelerator.device)\n",
    "\n",
    "    output_ids = model.generate(input_ids,\n",
    "                                max_new_tokens=gen_args['max_new_tokens'], \n",
    "                                pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    output = tokenizer.decode(output_ids[0], \n",
    "                              skip_special_tokens=True)\n",
    "\n",
    "    if gen_args['do_postprocess']:\n",
    "        output = postprocess(prompt, tokenizer, output, \n",
    "                             prompt_args['response_key'],\n",
    "                             prompt_args['end_key'], \n",
    "                             gen_args['return_full'])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Load generation config.\")\n",
    "    parser.add_argument(\"--config_path\", type=str, required=True, help=\"Path to the YAML config file for generating.\")\n",
    "    parser.add_argument(\"--input_text\", type=str, default=None, help=\"The text input (question).\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Load the generation config file\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    model_args = cfg.model\n",
    "    prompt_args = cfg.prompt\n",
    "    gen_args = cfg.generate\n",
    "    device_args = cfg.device\n",
    "\n",
    "    if args.input_text:\n",
    "        prompt_args['input']['text'] = args.input_text\n",
    "\n",
    "    response = generate_response(model_args, prompt_args, gen_args, device_args)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T17:35:28.186909Z",
     "iopub.status.busy": "2025-02-24T17:35:28.186571Z",
     "iopub.status.idle": "2025-02-24T17:35:28.192299Z",
     "shell.execute_reply": "2025-02-24T17:35:28.191353Z",
     "shell.execute_reply.started": "2025-02-24T17:35:28.186881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/generate.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/generate.yaml\n",
    "model:\n",
    "  pretrained_model_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
    "  pretrained_tokenizer_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
    "  load_in_4bit: true\n",
    "  load_in_8bit: false\n",
    "  bnb_4bit_compute_dtype: null\n",
    "  bnb_4bit_quant_type: \"nf4\"\n",
    "  bnb_4bit_use_double_quant: false\n",
    "  bnb_4bit_quant_storage: \"uint8\"\n",
    "  torch_dtype: float32\n",
    "  attn_implementation: null\n",
    "  device_map: null\n",
    "  low_cpu_mem_usage: null\n",
    "  lora_adapter: # path to lora adapter\n",
    "\n",
    "prompt:\n",
    "    use_no_keys: false\n",
    "    use_examples: false\n",
    "    use_context: false\n",
    "    intro_text: \"You are an expert in summarizing dialogue.\"\n",
    "    instruction:\n",
    "        key: \"### Instruction:\"\n",
    "        text: \"Summarize the following dialogue:\"\n",
    "    input:\n",
    "        key: \"### Dialogue:\"\n",
    "        text: |\n",
    "            \"\"\"Hannah: Hey, do you have Betty's number?\n",
    "            Amanda: Lemme check\n",
    "            Hannah: <file_gif>\n",
    "            Amanda: Sorry, can't find it.\n",
    "            Amanda: Ask Larry\n",
    "            Amanda: He called her last time we were at the park together\n",
    "            Hannah: I don't know him well\n",
    "            Hannah: <file_gif>\n",
    "            Amanda: Don't be shy, he's very nice\n",
    "            Hannah: If you say so..\n",
    "            Hannah: I'd rather you texted him\n",
    "            Amanda: Just text him 🙂\n",
    "            Hannah: Urgh.. Alright\n",
    "            Hannah: Bye\n",
    "            Amanda: Bye bye\"\"\"\n",
    "\n",
    "    response_key: \"### Summary:\"\n",
    "    end_key: null\n",
    "\n",
    "\n",
    "generate:\n",
    "  use_vllm: False\n",
    "  do_postprocess: false\n",
    "  return_full: false\n",
    "  skip_special_tokens: true\n",
    "  max_new_tokens: 512\n",
    "  temperature: 0\n",
    "  do_sample: null\n",
    "  top_p: null\n",
    "  # num_beams: 4,    \n",
    "  # num_return_sequences: 4,\n",
    "  # return_dict_in_generate: True,\n",
    "  # output_scores: True,\n",
    "  \n",
    "\n",
    "device:\n",
    "  use_cpu: false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T17:35:28.435445Z",
     "iopub.status.busy": "2025-02-24T17:35:28.435210Z",
     "iopub.status.idle": "2025-02-24T17:37:10.916228Z",
     "shell.execute_reply": "2025-02-24T17:37:10.915073Z",
     "shell.execute_reply.started": "2025-02-24T17:35:28.435424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-24 17:35:39.265411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-24 17:35:39.467182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-24 17:35:39.526424: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:09<00:00, 17.47s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "You are an expert in summarizing dialogue.\n",
      "\n",
      "### Instruction:\n",
      "Summarize the following dialogue:\n",
      "\n",
      "### Dialogue:\n",
      "\"\"\"Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\"\"\"\n",
      "\n",
      "\n",
      "### Summary:\n",
      "Summary of the dialogue: Amanda helps Hannah obtain Betty's phone number by suggesting she ask Larry, who called Betty last time they were at the park together. Initially hesitant, Hannah eventually agrees to text Larry for the information. The conversation ends with Amanda encouraging Hannah and Hannah reluctantly agreeing. The dialogue concludes with a farewell from both parties.\n",
      "Certainly! Here is a summary of the dialogue:\n",
      "\n",
      "Amanda assists Hannah in finding Betty's phone number by suggesting she ask Larry, who had recently contacted Betty. Hannah is initially reluctant but eventually agrees to text Larry for the information. The conversation ends with Amanda encouraging Hannah, and both parties saying goodbye.\n"
     ]
    }
   ],
   "source": [
    "!python generate.py --config_path=configs/generate.yaml"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
