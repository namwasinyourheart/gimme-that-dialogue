exp_manager:
  prj_name: llm_dialogue-sum
  exp_name: "flan-t5-base__samsum"
  seed: 202502
  task_name: 'dialogue-summarization'
  dataset_name: 'samsum'
  model_name: 'google/flan-t5-base'
  phase_name: 'eval'
  print_cfg: true

prepare_data:
  dataset:
    is_prepared: false # Check if dataset is ready or not
    data_path: Samsung/samsum
    is_dataset_dict: true
    id_col: 'id'
    input_col: "dialogue"
    output_col: "summary"
    context_col: 
    do_split: true
    subset_ratio: 1
    val_ratio: 0.25
    test_ratio: 0.2
    columns_to_retain: ['id', 'summary', 'text']
    do_save: true
    do_show: false
    prepared_data_path: ./exps/flan-t5-base__samsum/data/flan-t5-base__samsum.pkl
  
  prompt:
    use_model_chat_template: false
    use_only_input_text: false
    use_examples: false
    use_context: false

    intro_text: "Summarize the following dialogue."
    instruction_key: 
    instruction_text: 
    
    examples_key:
    examples_template: 
    examples_list: 
    
    context_key: null
    input_key: "### Dialogue:"
    response_key: "### Summary:"
    end_key: null
  
  tokenizer:
    new_pad_token: null
    do_tokenize: true
    truncation: true
    padding: max_length
    padding_side: 'left'
    add_special_tokens: false
    max_length: 512

prepare_model: 
  pretrained_model_name_or_path: google/flan-t5-base
  pretrained_tokenizer_name_or_path:
  load_in_4bit: false
  load_in_8bit: false
  bnb_4bit_compute_dtype: null
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_storage: "uint8"
  torch_dtype: float32
  attn_implementation: null
  device_map: null
  low_cpu_mem_usage: null
  lora_adapter: # path to lora adapter

train:
    use_peft: True
    lora:
      r: 64
      lora_alpha: 32
      lora_dropout: 0.0
      bias: none
      task_type: CAUSAL_LM
      inference_mode: false
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      modules_to_save:

    do_merge: true
    train_n_samples: 100
    val_n_samples: 10
    test_n_samples: 10
    
    train_args:
      _target_: transformers.TrainingArguments
      resume_from_checkpoint: 
      do_train: false
      do_eval: true
      do_predict: false
      learning_rate: 0.0001
      num_train_epochs: 1
      # max_steps: 1
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      # logging_strategy: "no"
      logging_steps: 1
      logging_first_step: true
      save_strategy: epoch
      eval_strategy: steps
      eval_steps: 50
      eval_accumulation_steps: 1
      eval_on_start: true
      use_cpu: false
      report_to: None

generate:
  max_new_tokens: 128
  pad_token_id: null
  skip_special_tokens: True
  do_sample:
  temperature: 


eval:
  batch_size: 36
  break_step: -1
  do_extract_prediction: False
  prediction_file: 'test_predictions_0shot.txt'
  result_file: 'test_result_0shot.txt'

device:
    use_cpu: False
